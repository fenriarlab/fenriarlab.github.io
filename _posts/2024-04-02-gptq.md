---
title: LLM Acceleration Techniques--GPTQ for LLM Quantization
tags: LLM Quantization
---

## Optimal Brain Damage (OBD)

The essence of Optimal Brain Damage (OBD) lies in the minimization of the objective function, which denotes the model training loss, through iterative selection of pruning parameters. In essence, let us denote the model weights as $W$, and the loss function as $f$. Then, given the current weights, the training loss of the model can be represented as $L = L(W)$.

Pruning the model essentially entails adjusting a value of $W$ to 0. Utilizing the Taylor series expansion of the loss function enables us to estimate the impact of weight adjustments. By expanding $L(W)$ via the Taylor series, we derive:

$$
\begin{aligned}
  &L(W + \delta W) = L(W) +  \left(\frac{\partial L}{\partial W}\right)^\top \delta W + \frac{1}{2} \delta W^\top H \delta W + \mathcal{O}(\delta W^3)\\
  \Rightarrow & \delta L = \left(\frac{\partial L}{\partial W}\right)^\top \delta W + \frac{1}{2} \delta W^\top H \delta W + \mathcal{O}(\delta W^3)
\end{aligned}\qquad (1)
$$

Here, $H$ denotes the Hessian matrix. $\delta L$ signifies the alteration in loss subsequent to pruning. Clearly, a smaller $\delta L$ is preferable. Thus, our aim is to identify a $\delta W$ that minimizes $\delta L$.

A well-trained neural network model typically resides in a local minimum within the weight space, allowing us to assume $\frac{\partial f}{\partial W} = 0$. Consequently, we may disregard the first term in the equation. Additionally, by neglecting higher-order terms, the equation simplifies to:

$$
\delta L = \frac{1}{2} \delta W^\top H \delta W \qquad\qquad (2)
$$

Next, it is imperative to introduce a significant assumption of OBD. To further streamline the problem, OBD posits that the Hessian matrix is a diagonal matrix. This assumption implies that the cumulative impact of pruning multiple weight parameters on model accuracy is tantamount to the summation of the effects of pruning each weight individually. In other words:

$$
\delta L = \frac{1}{2} \sum_i \delta w_i^2 h_{ii}
$$

Hence, the ultimate problem to address becomes:

$$
i = \arg\min_i \frac{1}{2} \delta w_i^2 h_{ii}
$$

Here, $i$ denotes the index of the weight parameter necessitating pruning in $W$.

## Optimal Brain Surgery (OBS)

Inspired by the foundational principles of Optimal Brain Damage (OBD), which aimed to minimize the repercussions of model pruning on the loss function, Optimal Brain Surgery (OBS) diverges from OBD's premise that weight prunings are independent. OBS contends that there exists correlation among weight prunings, thus refuting the assumption of a diagonal Hessian matrix. Hence, commencing from Equation (2), the analytical exploration unfolds:

$$
\delta L = \frac{1}{2} \delta W^\top H \delta W \qquad\qquad (2)
$$

Suppose we prune the $q$-th parameter, i.e., $\delta w_q + w_q = 0$, then it evolves into a constrained convex optimization problem:

$$
  \begin{aligned}
  &\arg\min_q \frac{1}{2} \delta W^\top H \delta W \\ 
  &s.t.\quad  \delta w_q + w_q = 0
  \end{aligned}
$$

Where the constraint can be expressed more generally as $\mathbf{e}_q^\top \delta W + w_q = 0$, with $\mathbf{e}_q$ denoting the unit vector with the $q$-th value as 1. For the aforementioned problem, the Lagrange multiplier method proves instrumental. Introducing the Lagrangian function:

$$
\mathcal{L} = \frac{1}{2} \delta W^\top H \delta W + \lambda (\mathbf{e}_q^\top \delta W + w_q)
$$

Taking derivatives with respect to $\delta W$ and $\lambda$, and setting them to 0:

$$
\begin{aligned}
\delta W^\top H  + \lambda \mathbf{e}_q^\top = 0\\
\mathbf{e}_q^\top \delta W  + w_q = 0
\end{aligned}
$$

We then execute the following transformations:

1.. Multiply one equation by $H^{-1}$, swap $\delta W$ and $\mathbf{e}_q$ in the second equation:

$$
\begin{aligned}
\delta W^\top H H^{-1}  + \lambda \mathbf{e}_q^\top H^{-1} = 0\\
\delta W^\top \mathbf{e}_q  + w_q = 0
\end{aligned}
$$

2.. Multiply the second equation by $\mathbf{e}_q$:

$$
\begin{aligned}
\delta W^\top \mathbf{e}_q  + \lambda \mathbf{e}_q^\top H^{-1} \mathbf{e}_q = 0\\
\delta W^\top \mathbf{e}_q  + w_q = 0
\end{aligned}
$$

<!-- $\mathbf{e}_q^T H^{-1} \mathbf{e}_q = {H^{-1}}_{qq}$ -->
${H^{-1}}_{qq}$

$H_{qq}^{-1}$

$[H]^{-1}_{qq}$

3.. Observe that , ${H^{-1}}_{qq}$, heh $H_{qq}^{-1}$ and substitute the second equation into the first equation, yielding:

$$
-w_q + \lambda [H^{-1}]_{qq}= 0\\
$$

4.. Solve for:

$$
\lambda = \frac{w_q}{[H^{-1}]_{qq}}
$$

5.. Substitute $\lambda$ into the first equation, and solve for:

$$
\delta W^\top = - \frac{w_q}{[H^{-1}]_{qq}} \mathbf{e}_q^\top H^{-1} =- \frac{w_q}{[H^{-1}]_{qq}} H^{-1}_{:,q}  \qquad \qquad (3)
$$

Where $H^{-1}_{:,q}$ represents the $q$-th column of $H^{-1}$. Subsequently, after obtaining $\delta W$, we substitute it into Equation (2):

$$
\begin{aligned}
\delta L_q &= \frac{1}{2} \left(- \frac{w_q}{[H^{-1}]_{qq}} \mathbf{e}_q^\top H^{-1}\right) H \left(- \frac{w_q}{[H^{-1}]_{qq}} \mathbf{e}_q^\top H^{-1}\right)^\top\\
&= \frac{1}{2} \left(\frac{w_q}{[H^{-1}]_{qq}}\right)^2 \mathbf{e}_q^\top H^{-1} H  (H^{-1})^\top \mathbf{e}_q\\
&= \frac{1}{2}  \left(\frac{w_q}{[H^{-1}]_{qq}}\right)^2 [H^{-1}]_{qq}\\
&= \frac{1}{2}  \frac{w_q^2}{[H^{-1}]_{qq}}
\end{aligned}
$$

Consequently, we solve for $q$:

$$
q = \arg\min_{q} \frac{w_q^2}{[H^{-1}]_{qq}} \qquad \qquad (4)
$$

Utilizing Equation (4), we identify the current optimal pruning parameter, and then apply Equation (3) to calculate the adjustment for all weights. This constitutes one iteration. By alternating between (3) and (4), we continuously identify the weights that necessitate pruning until the pruning objective is fulfilled.

## Optimal Brain Compression (OBC)

### Complexity Analysis of OBC

In each iteration of OBC, the computation of the inverse matrix of the Hessian matrix becomes imperative. Let $d$ denote the total number of weight parameters. Consequently, the time complexity of computing the inverse matrix of the Hessian matrix is $O(d^3)$. Given that this computation is requisite in each iteration, the overarching time complexity of OBC escalates to $O(d^4)$. Clearly, for models encompassing hundreds of thousands or even millions of parameters, the computational overhead of this approach is formidable.

### Row-wise Weight Pruning Algorithm

The seminal work on OBC initially formulates the layerwise model pruning problem, wherein, for each layer, the pruning loss is conceptualized as:

$$
\delta L(f(X, W), f(X, \hat{W}))
$$

Here, $f$ denotes the forward function of a particular layer, $W$ and $\hat{W}$ signify the original and pruned weight matrices, respectively, and $X$ represents the input in matrix format. Specifically, for linear layers or convolutional layers, the forward function can be articulated as the matrix multiplication of the weight and input matrices, $f(X, W) = WX$. Furthermore, assuming the squared loss function defines $\delta L$, identifying the optimal compression scheme reduces to the ensuing optimization problem:

$$
\arg\min_{\hat{W}} || WX - \hat{W} X ||^2
$$

Moreover, if we decompose the weight matrix row-wise, the pruning loss can be decomposed as the summation of row-wise calculations:

$$
\delta L = \sum_{i=1}^{d_{\text{row}}} || W_{i,:} X - \hat{W}_{i,:} X ||^2
$$

Let $\delta L_i = || W_{i,:} X - \hat{W}_{i,:} X ||^2$, then $\delta L = \sum_{i=1}^{d_{\text{row}}} \delta L_i$, yielding:

$$
\delta L_i = || W_{i,:} X - \hat{W}_{i,:} X ||^2 = \sum_{k = 1}^N \left(  \sum_{j=1}^{d_{\text{col}}} (w_{ij} - \hat{w}_{ij}) x_{jk} \right)^2
$$

Here, $N$ represents the number of columns in $X$, and $d_{\text{col}}$ represents the number of rows in $X$ or the number of columns in $W$.

Given that in each iteration, only one weight in a row is pruned, thereby influencing solely the pruning loss of one row, we can consider $\delta L = \delta L_i$ during each iteration, where $i$ denotes the index of the row containing the weight to be pruned. Based on this observation, the Hessian matrix can be derived as:

$$
\begin{aligned}
H_{pq} &= \frac{\partial^2 \delta L_i}{\partial w_{ip} \partial w_{iq}} \\
&= \frac{\partial}{\partial w_{ip}} \sum_{k = 1}^N 2\left(  \sum_{j=1}^{d_{\text{col}}} (w_{ij} - \hat{w}_{ij}) x_{jk} \right) \frac{\partial}{\partial w_{iq}} \sum_{j=1}^{d_{\text{col}}} (w_{ij} - \hat{w}_{ij}) x_{jk} \\
&= \frac{\partial}{\partial w_{ip}} \sum_{k = 1}^N 2\left(  \sum_{j=1}^{d_{\text{col}}} (w_{ij} - \hat{w}_{ij}) x_{jk} \right) x_{qk} \\
&= 2\sum_{k=1}^N x_{pk} x_{qk}
\end{aligned}
$$

Expressed in matrix notation:

$$
H = 2 X X^\top \qquad \qquad (5)
$$

It is worth noting that this constitutes the Hessian matrix when pruning the $i$-th row weight, denoted as $H^{(i)}$. Although the initial Hessian matrix remains consistent for each row, as pruning progresses, the Hessian matrices pertinent to each row will undergo disparate alterations. Specifically, the Hessian matrix is solely dependent on $X$. Consequently, when pruning the $q$-th weight in one iteration, the $q$-th row of $X$ becomes redundant, and in the subsequent iteration, the $q$-th row and column need only be removed from $H^{(i)}$, thereby economizing on the construction time of $H^{(i)}$ on each occasion.

On the other hand, each iteration also requires the computation of the inverse Hessian matrix, which is an operation with a complexity of $O(d_{col}^3)$. However, there are still ways to simplify it. Let $H_F^{-1}$ denote the intermediate $H^{-1}$, where $F$ denotes the set of unquantized weight indices, and $H_{F/q}^{-1}$ denotes the matrix $H_F^{-1}$ after removing the $q$-th row and $q$-th column. Then the following formula holds:

$$
H^{-1}_{F/q} = \left(H_F^{-1} - \frac{1}{[H_F^{-1}]_{qq}} [H_F^{-1}]_{:,q} [H_F^{-1}]_{q,:}\right)_{-q} \qquad \qquad (6)
$$

The proof of this formula can be found in the original paper, so we will not elaborate on it here. By using this formula, the computational complexity of each iteration of the Hessian inverse matrix can be reduced to $O(d_{col}^2)$.

Based on the above discussion, we can summarize the steps of the optimized OBS algorithm as follows:

1. Calculate the initial Hessian matrix $H^{(i)}$ according to formula (5), and copy a copy for each row of the weight matrix;
2. Iterate through each row and calculate the optimal weight index $q_i$ according to formula (4), where $i$ is the row index and the corresponding Hessian matrix is $H^{(i)}$;
3. Apply formula (3) to calculate the weight correction of the current row, and then update the weight matrix;
4. Update the $i$-th row Hessian matrix $H^{(i)}$, and update the Hessian inverse matrix according to formula (6);
5. Repeat steps 2-4 until the pruning target is reached.

Obviously, the above process requires applying formula (4) $d_{row}$ times each iteration, and then selecting the best $q_i$. For example, suppose the 10th row is selected in the first iteration and the 5th row is selected in the second iteration, Since each iteration only affects the Hessian matrix of the current row, the calculations of the Hessian matrices of different rows are independent of each other. In other words, even if the 5th row is selected in the first iteration, it will not affect the index calculation of the 10th row. Therefore, the iterations between different rows can be performed in parallel.

### From OBS to OBQ

Both pruning and quantization entail modifications to weight values. However, while pruning directly sets weights to 0, quantization diminishes the numerical precision of weights, transitioning from floating-point representations such as fp32 to alternatives like fp16, int8, int4, among others. Consequently, the principles of Optimal Brain Surgery (OBS) can naturally extend to quantization, thus giving rise to Optimal Brain Quantization (OBQ). To reconcile pruning and quantization, the overarching framework is termed Optimal Brain Compression (OBC).

In the OBQ methodology, we commence with Equation (2):

$$
\delta L = \frac{1}{2} \delta W^\top H \delta W \qquad\qquad (2)
$$

In OBS, the corresponding constraint is $\delta w_q + w_q = 0$, implying that the modification amount of the weight at index $q$ is $-w_q$. However, under quantization circumstances, the scenario becomes more intricate. According to the Google Quantization Whitepaper, quantization entails the conversion of floating-point weight values to integers:

$$
w_Q = \text{clamp}(0, N - 1, \text{round}(\frac{w}{\text{scale}}) + z)
$$

Here, $scale$ denotes the scaling factor, $z$ signifies the zero point offset, and $N$ represents the maximum integer value. For instance, taking $scale=0.09655$, $z = 8$, and $N = 16$ as exemplars, the graph of the quantization function is depicted below:

![](/resources/2024-04-02-gptq/gptq_quant.png)

This function maps floating-point weights to integer values in stages. A well-trained model applies quantization transformations to all weight values and preserves corresponding parameters such as $\text{scale}$, $z$, $N$, etc., constituting the classic quantization process.

Concomitant with the quantization process, an inverse quantization operation exists to restore the original precision type:

$$
w_{Q_f} = \text{scale} \times (w_Q - z)
$$

Hence, the quantized weights effectively possess two equivalent representations. The first representation is the integer space transformed by the quantization function, while the second is the floating-point space transformed by the inverse quantization function. Evidently, in the floating-point space, weight parameters manifest as discrete points, delineating a quantization grid. For OBQ, when quantizing a weight value, the remaining weights necessitate adjustment to minimize the quantization loss. However, adjustments must occur in the floating-point space, as the remaining weights are floating-point numbers, precluding direct adjustments in the integer space. Introduce the function $quant$ to first quantize the weights, then inversely quantize, yielding the quantized value in the floating-point space:

$$
quant(w) = \text{scale} \times (\text{clamp}(0, N - 1, \text{round}(\frac{w}{\text{scale}}) + z) - z)
$$

Consequently, the constraint $\delta w_q + w_q = 0$ in the pruning operation transmutes into $\delta w_q + w_q = quant(w_q)$. Leveraging the Lagrange multiplier method anew to resolve the minimization problem of Equation (2), we derive:

$$
q = \arg\min_{q} \frac{(quant(w_q) - w_q)^2}{[H^{-1}]_{qq}} \qquad \qquad (7)
$$

$$
\delta W = - \frac{w_q - quant(w_q)}{[H^{-1}]_{qq}}  (H^{-1}_{:,q})^\top \qquad \qquad (8)
$$

Here, $\delta W$ denotes the correction amount of a row of weights, initially of size $1 \times d_{col}$. Iterating between Equations (7) and (8) furnishes the quantized rendition of the OBC algorithm, which is named OBQ.

## GPTQ

### Quantization in Index Order

Optimal Brain Quantization (OBQ) employs a row-wise weight quantization technique, reducing the complexity associated with inverting the Hessian matrix to $O(d_{col}^2)$, where the total parameter count of a weight matrix is $d_{row} \times d_{col}$. Consequently, the overall time complexity of OBQ amounts to $O(d_{row} \times d_{col}^3)$. While this marks a significant advancement compared to the $O((d_{row}\times d_{col})^4)$ complexity of Optimal Brain Surgery (OBS), for large-scale models, OBQ remains a computationally intensive operation.

GPTQ proposed two enhancements to augment the quantization efficiency of OBQ. Firstly, rather than employing a greedy strategy for selecting quantized weights for each row, weights are chosen in index order. Secondly, the weight updating method is altered to batch updating. Subsequently, we elucidate that the latter is contingent on the former.

![](/resources/2024-04-02-gptq/gptq_order.png)

In the OBQ algorithm, Equation (7) is utilized to compute the optimal weights for the current row in each iteration. The authors of GPTQ posit that although this greedy strategy can yield high accuracy, the enhancement relative to selecting weights in any order is negligible. If all rows quantize weights in the same order, the quantization process can be substantially simplified.

Referring to the preceding Equation (5):

$$
H = 2 X X^\top \qquad \qquad (5)
$$

It is discernible that for each row of the weight matrix, its initial Hessian matrix is solely related to the input matrix $X$, rendering them identical. In OBQ, the optimization order of weights in each row may vary, necessitating separate storage of corresponding Hessian matrices for each row. However, when all rows quantize weights in the same order, the Hessian matrix for each row is uniform, thereby rendering the inverse matrix uniform as well. Consequently, solely $d_{col}$ inverse matrices need computation instead of $d_{col}\times d_{row}$ times, thereby reducing the overall time complexity to $O(d_{col}^3)$, and obviating the computation process of identifying the optimal weights in Equation (7).

Given that the quantization of weights for each row is executed sequentially from 0 to $d_{col}$, the inverse matrix of Hessian used when quantizing the $q$-th weight can be denoted as $[H_{q:, q:}]^{-1}$. Hence, the formula for computing the weight adjustment amount in Equation (8) can be revised as follows:

$$
\delta W = - \frac{w_q - quant(w_q)}{[H_{q:, q:}]^{-1}_{0,0}}  ([H_{q:, q:}]^{-1}_{:,0})^\top \qquad \qquad (9)
$$

Additionally, the update formula for the inverse Hessian matrix in Equation (6) can be reformulated as:

$$
[H_{q:,q:}]^{-1} = \left([H_{q-1:,q-1:}]^{-1} - \frac{1}{[H_{q-1:,q-1:}]^{-1}_{00}} [H_{q-1:,q-1:}]^{-1}_{:,0} [H_{q-1:,q-1:}]^{-1}_{0,:}\right)_{1:,1:} \qquad \qquad (10)
$$

Given that all rows quantize weights in the same order, one iteration can quantize all rows of the same column. Thus, we can devise a vectorized weight adjustment formula:

$$
\delta W = -\frac{W_{:, q} - quant(W_{:, q})}{[H_{q:, q:}]^{-1}_{0,0}} ([H_{q:, q:}]^{-1}_{:,0})^\top \qquad \qquad (11)
$$

Here, $W_{:, q}$ denotes the $q$-th column of the weight matrix, with a size of $d_{row}\times 1$.

### Cholesky Decomposition

GPTQ employs Cholesky decomposition to mitigate the numerical stability challenge encountered in computing the inverse matrix of $H$. During experimentation, the original authors observed that repeated application of Equation (10) on large-scale parameter matrices often yielded non-positive definite Hessian inverse matrices. This phenomenon might stem from accumulated numerical errors, culminating in the failure of the quantization process. To address this concern, the authors employ Cholesky decomposition on the initial $H^{-1}$ to derive an upper triangular matrix $T$. This matrix $T$ bears intriguing characteristics; each of its rows precisely matches the first row of the matrix sequence obtained by iteratively applying Equation (10), scaled by a constant. Formally:

$$
C_q T_{q, q:} =  [H_{q:, q:}]^{-1}_{0,:}
$$

This relationship can be visually represented as depicted below:

![](/resources/2024-04-02-gptq/gptq_cd.png)

Considering Equation (11), the computational process can be illustrated as shown in the subsequent figure:

![](/resources/2024-04-02-gptq/gptq_delta-w.png)

It is evident that when quantizing the $q$-th column of weights, solely the elements of the current Hessian inverse matrix column 0 are utilized, i.e., $[H_{q:, q:}]^{-1}_{:, 0}$. Given the symmetry of the Hessian matrix, this equals $[H_{q:, q:}]^{-1}_{0, :}$, which equivalently matches $C_q T_{q, q:}$.

From the foregoing analysis, it transpires that Equation (10) need not be applied. By solely leveraging the $T$ matrix derived from Cholesky decomposition, all information pertaining to the Hessian inverse matrix in the quantization process can be attained. Exploiting the connection between $T$ and the Hessian inverse matrix, we can adapt Equation (11) to utilize the representation involving $T$:

$$
\delta W = -\frac{W_{:, q} - quant(W_{:, q})}{C_q T_{qq}} C_q T_{q, q:} \qquad \qquad (12)
$$

It's worth noting that the constant disparity between $T$ and the Hessian inverse matrix is nullified in the aforementioned equation.

### Batch Quantization

Previously, it was posited that the sequencing of weight quantization has negligible influence on the ultimate quantization outcome. Consequently, simultaneous quantization of weights from multiple columns aligns well with this presumption. Therefore, GPTQ introduces a batch quantization methodology, specifically the batch variant of Equation (12), as depicted below:

![](/resources/2024-04-02-gptq/gptq_batch.png)

The complexity in this procedure arises from the diminishing size of the corresponding Hessian inverse matrix as each row undergoes quantization. Consequently, in batch computation, the $T_{q,q:}$ vector diminishes incrementally, rendering it unsuitable for direct matrix multiplication. To mitigate this challenge, we consider the decomposition portrayed in the ensuing figure:

![](/resources/2024-04-02-gptq/gptq_batch-decompose.png)

Following decomposition, the initial $B-1$ iterations are utilized to compute the weight adjustment for the pertinent columns, while the final iteration is dedicated to determining the weight adjustment for the residual columns. This strategy notably diminishes the frequency of weight matrix access, thereby enhancing computational efficiency.

## Summary

The aforementioned delineates the comprehensive evolution of GPTQ quantization methodology. In essence, Optimal Brain Damage (OBD) laid the foundation by leveraging second-order insights from pruning losses to select optimal weights. Subsequently, Optimal Brain Surgery (OBS) formulated a convex optimization problem based on this concept to ascertain weight adjustments. Optimal Brain Quantization (OBQ) then applied the tenets of OBS to the model quantization process. Ultimately, GPTQ introduced several optimization strategies to expedite the quantization procedure, facilitating the effective deployment of quantization technology even in the context of large-scale models.

## References

* LeCun, Yann, John Denker, and Sara Solla. "Optimal brain damage." Advances in neural information processing systems 2 (1989).

* Hassibi, Babak, David G. Stork, and Gregory J. Wolff. "Optimal brain surgeon and general network pruning." IEEE international conference on neural networks. IEEE, 1993.

* Frantar, Elias, and Dan Alistarh. "Optimal brain compression: A framework for accurate post-training quantization and pruning." Advances in Neural Information Processing Systems 35 (2022): 4475-4488.

* Frantar, Elias, et al. "Gptq: Accurate post-training quantization for generative pre-trained transformers." arXiv preprint arXiv:2210.17323 (2022).