---
title: The Derivation of Perplexity in Language Models
tags: Perplexity LLM
---

Perplexity is one of the key metrics used to evaluate the quality of language model generation. This article comprehensively demonstrates how to calculate the perplexity of a language model, covering concepts, formulas, and code implementation. This will help us quantitatively evaluate the generation quality of a specific language model for a given task, as well as assess the quality degradation after model quantization.

## Probability Mass Function and Cumulative Distribution Function of Discrete Random Variables

Consider a discrete random variable $X$ characterized by its probability mass function $p(x)$. This function, denoted by $p(x)$, assigns probabilities to individual outcomes $x_i$ in the sample space $\mathcal{X}$, where $P(X = x\_i) = p(x\_i)$ for any $x_i \in \mathcal{X}$. 

The cumulative distribution function $F_X(x)$ of $X$ is defined as the probability that $X$ takes on a value less than or equal to $x$, expressed as:

$$
F_X(x) = P(X \leq x) = \sum_{x_i \leq x} p(x_i)
$$

Conversely, if the cumulative distribution function $F_X(x)$ of $X$ is known, the probability mass function of $X$ can be obtained by differencing consecutive values of $F_X(x)$:

$$
p(x_i) = F_X(x_i) - F_X(x_{i - 1})
$$

## Empirical Distribution Function

The empirical distribution function serves as an approximation to the actual cumulative distribution function, derived from a given sample set. Specifically, for a sample set denoted as $\{x\_i\}\_1^{n}$, arranged in ascending order such that $x_1 < x_2 < ... < x_n$, the empirical distribution function $\hat{F}_n(x)$ is formulated as follows:

$$
\hat{F}_n(x) = \left\{ 
\begin{aligned}
&0 ,\quad \text{if} \quad x < x_1 \\
&\frac i n ,\quad \text{if} \quad x_i \leq x < x_{i + 1}\\
&1 ,\quad \text{if} \quad x \geq x_n
\end{aligned}
\right.
$$

The rationale behind the effectiveness of this definition in approximating the true probability distribution can be elucidated by referencing the [Glivenkoâ€“Cantelli theorem](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem). This theorem establishes that as the sample size $n$ tends towards infinity, the empirical distribution function converges uniformly to the actual distribution. For a more intuitive grasp, consider an example below: multiple points are sampled from a standard normal distribution, depicted by the red pulse points in the accompanying graph. The resulting empirical distribution function, represented by the blue line, notably aligns closely with the cumulative distribution function of the standard normal distribution (i.e., the black curve).

![](/resources/2024-04-21-ppl/ppl-empirical_distribution.png)

## Probability Mass Function of Empirical Distribution

Analogous to the relationship observed between the genuine probability mass function and cumulative distribution function, the computation of the probability mass function for the empirical distribution can be derived from the previously defined empirical distribution function. This relationship is expressed as:

$$
\hat{p}(x_i) = \hat{F}_n(x_i) - \hat{F}_n(x_{i - 1}) = \frac{1}{n}
$$

Alternatively, certain scholarly works may articulate the above equation in the following manner:

$$
\hat{p}(x) = \frac{1}{n} \{1\mid x = x_i\}
$$

Here, $\{1\mid x = x_i\}$ denotes the indicator function, assuming a value of 1 when $x = x_i$ and 0 otherwise.

## Information Content and Entropy

Consider a random variable $X$ characterized by its probability mass function $p(X)$. The information content associated with the event $X=x$ is defined as:

$$
I(x) = -\log_2 p(x)
$$

Interpreting this equation with a base-2 logarithm, it signifies the number of bits requisite for encoding $x$ within binary data. For instance, when $p(x) = 1$, indicating the certainty of $x$ occurring without the need for message encoding, its information content is 0. Conversely, when $p(x) = 0.5$, signifying equal probabilities for the occurrence and non-occurrence of $x$, $I(x) = 1$, indicating the necessity of 1 bit for encoding $x$, where 0 represents non-occurrence and 1 represents occurrence.

Entropy, denoted as $H(X)$, embodies the average information content pertaining to a random variable and is formulated as:

$$
H(X) = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
$$

Here, $\mathcal{X}$ denotes the sample space of $X$. As probability distributions inherently comprise events with varying probabilities, entropy serves as a metric for representing the average bit requirement for encoding an event within the distribution.

## Perplexity of Probability Distributions

[Perplexity](https://en.wikipedia.org/wiki/Perplexity) serves as a metric quantifying the level of uncertainty inherent in sampling from a discrete probability distribution. Higher perplexity values correspond to increased uncertainty in the sampling process. Notably, this definition exhibits a correlation with the concept of entropy. Specifically, for a probability distribution $p$ associated with a discrete random variable $X$, its perplexity is directly linked to its entropy $H(p)$, expressed by the following formula:

$$
PPL(p) = 2^{H(p)}
$$

## Cross Entropy

Consider two probability distributions, denoted as $p$ and $q$, defined on the same sample space. Here, $p$ signifies the genuine probability distribution, while $q$ represents an approximation thereof. When utilizing $q$ to gauge the information content (or encoding length) of an event $X=x$, the resulting measure is $I\_q(x) = -\log\_2 q(x)$. However, it's crucial to acknowledge that the actual probability associated with $X = x$ is $p(x)$. Consequently, employing $q$ to evaluate the average information content of a random variable yields:

$$
H(p, q) = -\sum_{x \in \mathcal{X}} p(x) \log_2 q(x)
$$

This measure denotes the cross entropy existing between $p$ and $q$. Notably, when $p$ equals $q$, the cross entropy reduces to $H(p)$.

## Perplexity of Probability Models

A probability model serves as a representation of the underlying true probability distribution. Consequently, the perplexity attributed to a probability model can be construed as an approximation to the perplexity inherent in the genuine probability distribution. When the probability model impeccably aligns with the true probability distribution, the perplexity of the probability model coincides with that of the true distribution. In this context, the perplexity of a probability model can be conceptualized utilizing the cross entropy between the probability model and the genuine probability distribution, as depicted by the formula:

$$
PPL(q) = 2^{H(p, q)} 
$$

Given the inherent unknown nature of the true probability distribution, the empirical distribution serves as a surrogate. Referring back to earlier derivations, for a discrete random variable $X$ with a sample space size denoted as $n = \mid\mathcal{X}\mid$, the probability mass function of its empirical distribution is expressed as $\hat{p}(X = x) = \frac{1}{n}$. Consequently, we derive:

$$
\begin{aligned}
H(\hat{p},q) &= -\sum_{x \in \mathcal{X}} \hat{p}(x) \log_2 q(x)\\
&= -\frac 1 n \sum_{x \in \mathcal{X}} \log_2 q(x)
\end{aligned}
$$

Substituting into the formula for PPL, we get

$$
\begin{aligned}
PPL(q) &= 2^{-\frac 1 n \sum_{x\in \mathcal{X}} \log_2 q(x)} \\
&= 2^{-\frac 1 n \log_2 \prod_{x\in\mathcal{X}} q(x)} \\
&= \left(\prod_{x\in\mathcal{X}} q(x)\right)^{-1/n}
\end{aligned}
$$

## Joint Entropy

[Joint entropy](https://en.wikipedia.org/wiki/Joint_entropy) serves as a metric quantifying the entropy within the joint distribution of a group of random variables. It is formally defined as:

$$
H_m(X_1, X_2,..., X_m) = -\sum_{x_i \in\mathcal{X},i=1...m} p(x_1, x_2, ..., x_m) \log_2 p(x_1, x_2, ..., x_m)
$$

Here, $\mathcal{X}$ denotes the sample space, and $p(x\_1, x\_2, ..., x\_m)$ represents the joint probability distribution of $X\_1, X\_2,..., X\_m$.

## Entropy Rate

The [entropy rate](https://en.wikipedia.org/wiki/Entropy_rate) pertains to the entropy associated with a stochastic process. For a stochastic process denoted as $\theta$, corresponding to a sequence of random variables $X\_1, X\_2,..., X\_m$, let the joint entropy of this sequence be designated as $H_m(X\_1, X\_2,..., X\_m)$. The entropy rate of $\theta$ is then defined as:

$$
\begin{aligned}
H(\theta) &= \lim_{m\rightarrow \infty} \frac{1}{m} H_m(X_1, X_2,..., X_m)\\
&= - \lim_{m\rightarrow \infty} \frac{1}{m} \sum_{x_i\in\mathcal{X}, i=1...m} p_\theta(x_1, x_2, ..., x_m) \log_2 p_\theta(x_1, x_2, ..., x_m)
\end{aligned}
$$

## Asymptotic Equipartition Property (AEP)

Viewing $-\log\_2{p\_\theta(x\_1,x\_2,...,x\_m)}$ as the information content of the sequence $x\_1,x\_2,...x\_m$, the entropy rate $H(\theta)$ embodies the anticipated information content of the sequence $X\_1,X\_2,...X\_m$ across all feasible values. It's essential to emphasize that this metric pertains to the totality of conceivable values within infinitely long sequences. In accordance with the [Shannon-McMillan-Breiman theorem](https://en.wikipedia.org/wiki/Asymptotic_equipartition_property), if a stochastic process satisfies both [ergodicity](https://en.wikipedia.org/wiki/Ergodic_process) and [stationarity](https://en.wikipedia.org/wiki/Stationary_process) conditions, then for any infinitely long sequence $X_1, X_2, ..., X_m$, the negative logarithm of the joint distribution probability divided by $m$ converges to the entropy rate, illustrated by the expression:

$$
-\lim_{m\rightarrow \infty} \frac{1}{m}\log_2 p_\theta(X_1, X_2, ..., X_m) = H(\theta)
$$

## Cross Entropy between Stochastic Processes

Analogous to the concept of cross entropy between probability distributions, we extend this notion to define the cross entropy between stochastic processes based on their entropy rates. Consider two stochastic processes denoted as $\theta$ and $\xi$. The cross entropy concerning the sequence of random variables $X\_1, X\_2,..., X\_m$ is expressed as:

$$
H(\theta, \xi) =- \lim_{m \rightarrow \infty}\frac{1}{m} \sum_{x_i \in \mathcal{X}, i=1...m} p_\theta(x_1, x_2, ..., x_m) \log_2 p_\xi(x_1, x_2, ..., x_m)
$$

Furthermore, if $\theta$ satisfies the conditions of ergodicity and stationarity, in accordance with the asymptotic equipartition property, the aforementioned equation can be simplified to:

$$
H(\theta, \xi) =- \lim_{m \rightarrow \infty} \frac{1}{m} \log_2 p_\xi(X_1,X_2, ...,X_m)
$$

## Perplexity of Stochastic Processes

A stochastic process manifests as a probability distribution at each temporal point. Therefore, akin to utilizing entropy for defining the perplexity of probability distributions, we employ entropy rates to define the perplexity of stochastic processes. For a stochastic process $\theta$, the perplexity is articulated as:

$$
PPL(\theta) = 2^{H(\theta)}
$$

## Language Models

Language models can be construed as a collection of probability models, with each model at a specific temporal point encapsulating the language probability distribution of the real world. Precisely, when presented with a sequence of tokens, a language model can estimate the probability of the subsequent token's occurrence, denoted as $p_\xi(X\_i \mid X\_{<i})$. Moreover, leveraging the conditional probability formula, the language model is equipped to calculate the probability of the entire sentence:

$$
p_\xi(X_1, X_2, ..., X_m) = p_\xi(X_1)p_\xi(X_2\mid X_1)p_\xi(X_3\mid X_1, X_2)...p_\xi(X_m\mid X_1, X_2, ..., X_{m-1})
$$

## Perplexity of Language Models

At each temporal step, a language model serves as a representation of the underlying real-world language probability distribution. Consequently, the language model can be conceptualized as a model of the real-world language stochastic process. Analogous to the determination of perplexity for probability models, the perplexity of a language model is derived from the cross entropy between the language model itself and the actual real-world language stochastic process, expressed as:

$$
PPL(\xi) = 2^{H(\theta, \xi)}
$$

Should the language model concurrently satisfy the conditions of stationarity and ergodicity, then $H(\theta, \xi)$ can be articulated as:

$$
\begin{aligned}
H(\theta, \xi) &=- \lim_{m \rightarrow \infty} \frac{1}{m} \log_2 p_\xi(X_1,X_2, ...,X_n) \\
&= -\lim_{m \rightarrow \infty} \frac{1}{m} \log_2 \prod_{i=1}^m p_\xi(X_i \mid X_{<i}) \\
&= -\lim_{m \rightarrow \infty} \frac{1}{m} \sum_{i=1}^m \log_2 p_\xi(X_i \mid X_{<i})
\end{aligned}
$$

* Note: Regarding the utilization of the asymptotic equipartition property to simplify the cross entropy, a specific proof process wasn't found. Most references merely mention it briefly, but the conclusion is deemed accurate.

## Calculation of Perplexity for Autoregressive Language Models

In the preceding discussion, we explored the perplexity of language models from a theoretical perspective. Now, let's consider how to calculate the perplexity of an autoregressive language model. An autoregressive language model refers to a model trained using autoregressive methods. Specifically, it predicts the next token based on the preceding tokens in the token sequence, compares it with the actual token, and computes the cross-entropy loss. This is a typical unsupervised learning method, and currently, almost all generative language models are trained using this approach. Therefore, here we only discuss this type of language model.

Based on the derived formula for cross-entropy calculation:

$$
H(\theta, \xi) = -\lim_{m \rightarrow \infty} \frac 1 m \sum_{i=1}^m \log_2 p_\xi(X_i \mid X_{<i})
$$

Here, $m$ needs to tend to infinity. However, in practical applications, it is clearly impossible to handle infinitely long sequences. Therefore, we can only use finite-length data for approximation. Additionally, autoregressive language models have a context length limitation. Therefore, the evaluation dataset needs to be segmented into multiple subsequences, each no longer than the context length. Then, we compute each subsequence separately and finally take the average. In other words:

$$
\begin{aligned}
H_j(\theta, \xi) &= -\frac 1 C \sum_{i=1}^C \log_2 p_\xi(X_{i+jC} \mid  X_{> jC ,\,< i + jC})\\
H(\theta, \xi) &= \frac 1 N \sum_{j=1}^N H_j(\theta, \xi)
\end{aligned}
$$

Here, $C$ represents the context length, and $N$ represents the number of subsequences.

For an autoregressive language model, given a token sequence, such as $[x\_0, x\_1,..., x\_{i-1}]$, the probability of predicting the next token $x_i$ is $p_\xi(x\_i\mid x\_{<i})$. From a code perspective, taking GPT-2 as an example, this is the result after the model's logits output is passed through softmax.

```python
with torch.no_grad():
    outputs = model(input_ids)
    logits = outputs.logits
    logits = logits[:, :-1] ## Logits for predicting tokens
    probs = F.softmax(logits, dim=-1)
```

For a given sequence, $x_i$ is known. If its id in the vocabulary is $k$, then the probability $p\_\xi(x\_i\mid x\_{<i})$ given by the language model is `probs[k]`.

Using the logits output by the model, we can compute all $p\_\xi(x\_i\mid x\_{<i}), i = 1,2,...,m$. Below, we illustrate the perplexity calculation for GPT-2 on the WikiText2 dataset, referencing the Hugging Face [perplexity](https://huggingface.co/docs/transformers/perplexity) page for this section.

```python
from datasets import load_dataset
from transformers import AutoTokenizer, GPT2LMHeadModel
from tqdm import tqdm
import torch
import numpy as np

tokenizer=AutoTokenizer.from_pretrained("gpt2")
model=GPT2LMHeadModel.from_pretrained("gpt2")
device = "cuda"
model.to(device)
model.eval()

test = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
encoding = tokenizer("\n\n".join(test["text"]), return_tensors="pt")

seq_len = encoding.input_ids.shape[1]
## context length
C = 1024

log_prob = []
for begin_loc in tqdm(range(0, seq_len, C)):
  end_loc = min(begin_loc + C, seq_len)
  input_ids = encoding.input_ids[:, begin_loc: end_loc].to(device)
  with torch.no_grad():
    outputs = model(input_ids)
    logits = outputs.logits
    logits = logits[:, :-1]
    labels = input_ids[:, 1:]
    probs = torch.softmax(logits, dim=-1)
    probs = probs.squeeze(0)
    labels = labels.squeeze(0)
    target_probs = torch.gather(probs, 1, labels.unsqueeze(1))
    log_prob.extend(target_probs.log2().cpu().numpy().tolist())
  
  if end_loc == seq_len:
    break  

ce = - np.sum(log_prob) / len(log_prob)
ppl = 2 ** ce
```

The final result is 29.94, which is close to the reported result of 29.41 in the GPT-2 paper. It is important to note that Hugging Face uses the model's output cross-entropy loss, while we explicitly wrote out the calculation process, which are equivalent.

## Summary

Starting from basic concepts, this article systematically derived the perplexity of probability distributions, the perplexity of random processes, and the perplexity of language models. Finally, we provided the method for calculating the perplexity of autoregressive language models.

## References

* https://www.seas.ucla.edu/spapl/weichu/htkbook/node218_mn.html

* https://www.statlect.com/asymptotic-theory/empirical-distribution

* https://www.mdpi.com/1099-4300/20/11/839

